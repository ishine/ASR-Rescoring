task: training

method: "n_best_align"

seed: 10

lr: 0.000001
epoch: 40
device: "cuda:0"

train_feature:
  - ref_token_ids
  - hyps_token_ids
train_feature_path: 
  - "../espnet_data/alfred/train/ref_text.json"
  - "../espnet_data/alfred/train/hyps_text.json"
dev_feature:
  - ref_token_ids
  - hyps_token_ids
dev_feature_path: 
  - "../espnet_data/alfred/dev/ref_text.json"
  - "../espnet_data/alfred/dev/hyps_text.json"

output_path: "result/3_best_align_lr_10-6"

# the unit of batch size is utterence
# if batch_size = m, means m*n-best hypotheses in one batch
<<<<<<< HEAD
batch_size: 10
=======
batch_size: 32
>>>>>>> 7726d8bed298638a68a9e4ca1c12932fa45c9693

# if you don't want to process too much data (like for debugging)
# you can set this value to a small number.
max_utt: 1
<<<<<<< HEAD
n_best: 4
=======
n_best: 3
>>>>>>> 7726d8bed298638a68a9e4ca1c12932fa45c9693

dataloader:
  shuffle: False
  num_worker: 5

model:
  bart: "fnlp/bart-base-chinese"
  alignment_embedding: 512

resume: 
  start_from: 31
  checkpoint_path: result/3_best_align_lr_10-6/checkpoint_30.pth